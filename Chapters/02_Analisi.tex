\chapter{Analisi di algoritmi}
Per definire la complessit\`a di un algoritmo occorre definire una funzione che ha come dominio la dimensione dell'input e come insieme immagine il tempo.
\subsubsection{Dimensione dell'input}
La dimensione dell'input pu\`o essere definita secondo due criteri:
\begin{itemize}
\item Criterio di costo logaritmico: la taglia dell'input \`e il numero di bit necessari a rappresentarlo.
\item Criterio di costo uniforme: la taglia dell'input \`e il numero di elementi di cui \`e costituito.
\end{itemize}
In molti casi si pu\`o assumere che gli elementi siano costituiti da un numero costante di bit, e in tal caso le due misure coincidono a meno di una costante
moltiplicativa.
\subsubsection{Definizione di tempo}
Si definisce il tempo come il numero di istruzioni elementari necessarie al completamento dell'algoritmo. Si definisce elementare un'istruzione che pu\`o 
essere svolta in tempo costante dal processore. 
\section{Modelli di calcolo}
Un modello di calcolo \`e una rappresentazione astratta del calcolatore. L'astrazione permette di nascondere dei dettagli, il suo realismo permette di 
riflettere con un certo grado di precisione una situazione reale e la potenza matematica di trarre conclusioni formali sul costo. 
\subsection{Macchina di Turing}
Una macchina di Turing \`e una macchina ideale che manipola i dati contenuti su un nastro di lunghezza infinita secondo un insieme prefissato di regole. Ad
ogni passo la macchina di Turing:
\begin{itemize}
\item Legge il simbolo sotto la testina.
\item Modifica il proprio stato interno.
\item Scrive un nuovo simbolo nella cella.
\item Muove la testina a destra o a sinistra.
\end{itemize}
Questo modello \`e fondamentale per lo studio della calcolabilit\`a ma \`e troppo dettagliato per l'analisi.
\subsection{Random Access Machine (RAM)}
\subsubsection{Memoria}
La memoria \`e costituita da un numero infinito di celle di dimensione finita a cui si accede, indipendentemente dalla posizione, in tempo costante.
\subsubsection{Processore (singolo)}
Un insieme di istruzioni simili a quelle reali: algebriche, logiche e di salto.
\subsubsection{Costo delle istruzioni elementari}
Uniforme e ininfluente ai fini dell'analisi. 
\section{Funzioni di costo, analisi asintotica}
Per studiare la complessit\`a di un algoritmo si analizza il suo comportamento asintotico, ovvero quando la dimensione del suo input tende a infinito.
\subsection{Notazione $\mathbf{O}$}
Sia $g(n)$ una funzione di costo, si indica con $O(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui:
\begin{equation*}
\exists c>0,\exists m\ge 0: f(n)\le cg(n),\forall n\ge m
\end{equation*}
$g(n)$ si dice limite asintotico superiore di $f(n)$, ovvero $f(n)$ cresce al pi\`u come $g(n)$.
\subsection{Notazione $\mathbf{\Omega}$}
Sia $g(n)$ una funzione di costo, si indica con $\Omega(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui:
\begin{equation*}
\exists c>0,\exists m\ge 0: f(n)\ge cg(n),\forall n\ge m
\end{equation*}
$g(n)$ si dice limite asintotico inferiore di $f(n)$, ovvero $f(n)$ cresce almeno quanto $g(n)$.
\subsection{Notazione $\mathbf{\Theta}$}
Sia $g(n)$ una funzione di costo, si indica con $\Theta(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui:
\begin{equation*}
\exists c_1>0,\exists c_2>0,\exists m\ge 0: c_1g(n)\le f(n)\le c_2g(n),\forall n\ge m
\end{equation*}
$f(n)$ cresce esattamente come $g(n)$ e $f(n)=\Theta(g(n))$ se e solo se $f(n)=O(g(n))$ e $f(n)=\Omega(g(n))$.
\subsection{Propriet\`a della notazione asintotica}
\subsubsection{Espressioni polinomiali}
\paragraph{Enunciato}
\begin{equation*}
f(n)=a_kn^k+a_{k-1}n^{k-1}+\cdots+a_1n+a_0, a_k>0\Rightarrow f(n)=\Theta(n^k)
\end{equation*}
\paragraph{Limite superiore}
\begin{equation*}
\exists c>0, \exists m\ge 0:f(n)\le cn^k,\forall n\ge m
\end{equation*}
\subparagraph{Dimostrazione}
\begin{align*}
f(n)&=a_kn^k+a_{k-1}n^{k-1}+\cdots+a_1n+a_0\\
&\le a_kn^k+|a_{k-1}|n^{k-1}+\cdots+|a_1|n+|a_0|\\
&\le a_kn^k+|a_{k-1}|n^k+\cdots+|a_1|n^k+|a_0|n^k\quad\quad\forall n\ge 1\\
&= (a_k+|a_{k-1}|+\cdots+|a_1|+|a_0|)n^k\\
&\overset{?}{\le} cn^k
\end{align*}
Vera per $c\ge (a_k+|a_{k-1}|+\cdots+|a_1|+|a_0|)$ e per $m=1$.
\paragraph{Limite inferiore}
\begin{equation*}
\exists d>0, \exists m\ge 0:f(n)\ge dn^k,\forall n\ge m
\end{equation*}
\subparagraph{Dimostrazione}
\begin{align*}
f(n)&=a_kn^k+a_{k-1}n^{k-1}+\cdots+a_1n+a_0\\
&\ge a_kn^k-|a_{k-1}|n^{k-1}-\cdots-|a_1|n-|a_0|\\
&\ge a_kn^k-|a_{k-1}|n^{k-1}-\cdots-|a_1|n^{k-1}-|a_0|n^{k-1}\quad\quad\forall n\ge 1\\
&\overset{?}{\ge} dn^k
\end{align*}
Vera per $d\le a_k-\frac{|a_{k-1}|}{n}-\frac{|a_{k-2}|}{n}-\cdots-\frac{|a_{1}|}{n}-\frac{|a_{0}|}{n}>0\Leftrightarrow n>\frac{|a_{k-1}|+\cdots+|a_0|}{a_k}$.
\subsubsection{Dualit\`a}
\paragraph{Enunciato}
\begin{equation*}
f(n)=O(g(n))\Leftrightarrow g(n)=\Omega(f(n))
\end{equation*}
\paragraph{Dimostrazione}
\begin{align*}
f(n)=O(g(n))&\Leftrightarrow f(n)\le cg(n),\forall n\ge m\\
&\Leftrightarrow g(n)\ge \frac{1}{c}f(n), \forall n\ge m\\
&\Leftrightarrow g(n)\ge c'f(n),\forall n\ge m, c'=\frac{1}{c}\\
&\Leftrightarrow g(n)=\Omega(f(n))
\end{align*}
\subsubsection{Eliminazione delle costanti}
\paragraph{Enunciato}
\begin{equation*}
f(n)=O(g(n))\Leftrightarrow af(n)=O(g(n)),\forall a>0
\end{equation*}
\begin{equation*}
f(n)=\Omega(g(n))\Leftrightarrow af(n)=\Omega(g(n)),\forall a>0
\end{equation*}
\paragraph{Dimostrazione}
\begin{align*}
f(n)=O(g(n))&\Leftrightarrow f(n)\le cg(n),\forall n\ge m\\
&\Leftrightarrow af(n)\le acg(n),\forall n\ge m,\forall a\ge 0\\
&\Leftrightarrow af(n)\le c'g(n),\forall n\ge m,c'=ac>0\\
&\Leftrightarrow af(n)=O(g(n))
\end{align*}
La dimostrazione \`e analoga per il limite inferiore.
\subsubsection{Sommatoria}
\paragraph{Enunciato}
\begin{equation*}
f_1(n)=O(g_1(n)), f_2(n)=O(g_2(n))\Rightarrow f_1(n)+f_2(n)=O(\max(g_1(n), g_2(n)))
\end{equation*}
\begin{equation*}
f_1(n)=\Omega(g_1(n)), f_2(n)=\Omega(g_2(n))\Rightarrow f_1(n)+f_2(n)=\Omega(\min(g_1(n), g_2(n)))
\end{equation*}
\paragraph{Dimostrazione}
\begin{align*}
f_1(n)=O(g_1(n))\land f_2(n)=O(g_2(n))&\Rightarrow\\
f_1(n)\le c_1g_1(n)\land f_2(n)\le c_2g_2(n)&\Rightarrow\\
f_1(n)+f_2(n)\le c_1g_1(n)+c_2g_2(n)&\Rightarrow\\
f_1(n)+f_2(n)\le \max(c_1, c_2)(2\max(g_1(n), g_2(n)))&\Rightarrow\\
f_1(n)+f_2(n)=O(\max(g_1(n), g_2(n))& 
\end{align*}
La dimostrazione \`e analoga per il limite inferiore.
\subsubsection{Prodotto}
\begin{equation*}
f_1(n)=O(g_1(n)), f_2(n)=O(g_2(n))\Rightarrow f_1(n)\cdot f_2(n)=O(g_1(n)\cdot g_2(n))
\end{equation*}
\begin{equation*}
f_1(n)=\Omega(g_1(n)), f_2(n)=\Omega(g_2(n))\Rightarrow f_1(n)\cdot f_2(n)=\Omega(g_1(n)\cdots g_2(n))
\end{equation*}
\paragraph{Dimostrazione}
\begin{align*}
f_1(n)=O(g_1(n))\land f_2(n)=O(g_2(n))&\Rightarrow\\
f_1(n)\le c_1g_1(n)\land f_2(n)\le c_2g_2(n)&\Rightarrow\\
f_1(n)\cdot f_2(n)\le c_1c_2g_1(n)g_2(n)&\Rightarrow\\
f_1(n)f_2(n)=O(g_1(n)g_2(n))
\end{align*}
\subsubsection{Simmetria}
\paragraph{Enunciato}
\begin{equation*}
f(n)=\Theta(g(n))\Leftrightarrow g(n)=\Theta(f(n))
\end{equation*}
\paragraph{Dimostrazione}
Grazie alla propriet\`a di dualit\`a:
\begin{align*}
f(n)=\Theta(g(n))&\Rightarrow & f(n)=O(g(n)) &\Rightarrow & g(n)=\Omega(f(n))\\
f(n)=\Theta(g(n))&\Rightarrow & f(n)=\Omega(g(n)) &\Rightarrow & g(n)=O(f(n))
\end{align*}
\subsubsection{Transitivit\`a}
\paragraph{Enunciato}
\begin{equation*}
f(n)=O(g(n)), g(n)=O(h(n))\Rightarrow f(n)=O(h(n))
\end{equation*}
\paragraph{Dimostrazione}
\begin{align*}
f(n)=O(g(n))\land g(n)=O(h(n))&\Rightarrow\\
f(n)\le c_1g(n)\land g(n)\le c_2h(n)&\Rightarrow\\
f(n)\le c_1c_2h(n)&\Rightarrow\\
f(n)=O(h(n))&
\end{align*}
\subsubsection{Logaritmi}
\paragraph{Enunciato}
Si vuole provare che $\log n=O(n)$. Si dimostri pertanto per induzione che:
\begin{equation*}
\exists c>0, \exists m\ge 0: \log n\le cn,\forall n\ge m
\end{equation*}
\paragraph{Dimostrazione}
\begin{itemize}
\item Caso base, $n=1$: $\log 1=0\le cn=c\cdot 1\Leftrightarrow c\ge 0$.
\item Ipotesi induttiva: sia $\log k\le ck, \forall k\le n$.
\item Passo induttivo: si dimostri la propriet\`a per $n+1$.
\begin{align*}
\log(n+1)&\le\log(n+n)=\log 2n\quad\quad &\forall n\ge 1\\
&=\log 2+\log n & \log ab=\log a+\log b\\
&=1+\log n & \log 2=1\\
&\le 1+cn & \text{per induzione}\\
&\overset{?}{\le} c(n+1) & \text{obiettivo}\\
1+cn\le c(n+1)\Leftrightarrow c\ge 1
\end{align*}
\end{itemize}
\subsection{Notazioni $\mathbf{o,\ \omega}$}
\subsubsection{Notazione $\mathbf{o}$}
Sia $g(n)$ una funzione di costo, si indica con $o(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui:
\begin{equation*}
\forall c, \exists m: f(n)<cg(n),\forall n\ge m
\end{equation*}
\subsubsection{Notazione $\mathbf{\omega}$}
Sia $g(n)$ una funzione di costo, si indica con $\omega(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui:
\begin{equation*}
\forall c, \exists m: f(n)>cg(n),\forall n\ge m
\end{equation*}
\subsubsection{Significato}
Utilizzando il concetto di limite si noti come:
\begin{itemize}
\item $\lim\limits_{x\rightarrow\infty}\dfrac{f(n)}{g(n)}=0\Rightarrow f(n)=o(g(n))$.
\item $\lim\limits_{x\rightarrow\infty}\dfrac{f(n)}{g(n)}\neq 0\Rightarrow f(n)=\Theta(g(n))$.
\item $\lim\limits_{x\rightarrow\infty}\dfrac{f(n)}{g(n)}=\infty\Rightarrow f(n)=\omega(g(n))$.
\item $f(n)=o(g(n))\Rightarrow f(n)=O(g(n))$.
\item $f(n)=\omega(g(n))\Rightarrow f(n)=\Omega(g(n))$.
\end{itemize}
\subsection{Classificazione delle funzioni}
Espandendo le relazioni dimostrate \`e possibile ottenere un ordinamento delle principali espressioni. Si consideri per ogni $r<s,\ h<k,\ a<b$:
\begin{align*}
O(1)&\subset O(\log^r n)\subset O(\log^s n)\subset O(n^h)\subset O(n^h\log^r n)\\
&\subset O(n^h\log^s n)\subset O(n^k)\subset O(a^n)\subset O(b^n)
\end{align*}
\subsection{Moltiplicare numeri binari}
\subsubsection{Algoritmo elementare del prodotto}
L'algoritmo \emph{prod()} moltiplica ogni bit con ogni altro bir per un costo totale di $cn^2$, ovvero $T_{prod}(n)=O(n^2)$.
\subsubsection{Algoritmo divide-et-impera}
\begin{align*}
	X &= a\cdot 2^{\frac{n}{2}}+ b\\
	Y &= c\cdot 2^{\frac{n}{2}}+ d\\
 X\cdot Y &= ac\cdot 2^n + (ad+bc)\cdot 2^{\frac{n}{2}} +bd
\end{align*}
\input{Pseudocodice/156_pdi}
Si noti come moltiplicare per $2^t$ \`e uno shift di $t$ posizioni in tempo lineare. 
$$T(n) = \begin{cases}
	c_1 & n = 1\\
	4T(\frac{n}{2}) + c_2\cdot n &n>1\\
\end{cases}$$
Espandendo l'equazione di ricorrenza: 
\begin{align*}
	&c_2\cdot1\cdot n\\
	&c_2\cdot 4\cdot \frac{n}{2}\\
	&c_2\cdot 4^2\cdot\frac{n}{2^2}\\
	&\vdots\\
	&c_2\cdot 4^i\cdot\frac{n}{2^i}\\
	&T(1)\cdot4^{\log n}\\
	&=c_1\cdot n^{\log 4}\\
	&=c_1\cdot n^2
\end{align*}
Si noti pertanto come $T(n) = O(n^2)$, pertanto non si \`e migliorato rispetto all'algoritmo elementare, anzi, sono state introdotte costanti moltiplicative pi\`u elevate. 
\subsubsection{Moltiplicazione di Karatsuba}
\begin{align*}
	A_1 &= a\times c\\
	A_3 &= b\times d\\
	m   &= (a+b)\times(c+d) = ac + ad+bc+bd\\
	A_2 &= m - A_1 - A_3 = ad+bc
\end{align*}
\input{Pseudocodice/157_Karatsuba}
$$T(n) = \begin{cases}
	c_1 & n = 1\\
	3T(\frac{n}{2}) + c_2\cdot n &n>1\\
\end{cases}$$
Espandendo l'equazione di ricorrenza: 
\begin{align*}
	&c_2\cdot1\cdot n\\
	&c_2\cdot 3\cdot \frac{n}{2}\\
	&c_2\cdot 3^2\cdot\frac{n}{2^2}\\
	&\vdots\\
	&c_2\cdot 3^i\cdot\frac{n}{2^i}\\
	&T(1)\cdot3^{\log n}\\
	&=c_1\cdot n^{\log 3}\\
	&=c_1\cdot n^{1.58\dots}
\end{align*}
Si noti pertanto come in questo caso $T_{kara}(n) = O(n^{1.58\dots})$.
\section{Ricorrenze}
\subsubsection{Definizioni}
\begin{itemize}
\item Equazione di ricorrenza: calcolare la complessit\`a di un algoritmo ricorsivo richiede le creazione di un'equazione di ricorrenza, una formula 
matematica definita in maniera ricorsiva.
\item Forma chiusa: l'obiettivo \`e partire dall'equazione di ricorrenza e trasformarla in una forma chiusa in modo da comprendere la classe di 
complessit\`a dell'algoritmo.
\end{itemize}
\subsection{Analisi per livelli}
Questo metodo di risoluzione delle equazioni ricorsive detto anche metodo dell'albero di ricorsione consiste nell'espandere la ricorrenza in un albero i cui
nodi rappresentano i costi ai vari livelli della ricorsione considerando poi il livello pi\`u basso in cui tutte le chiamate sono state ricondotte al caso
base. Si otterr\`a pertanto una sommatoria da cui si potr\`a derivare la forma chiusa e la classe di complessit\`a.
\subsection{Metodo della sostituzione}
Questo metodo di risoluzione delle equazioni ricorsive, detto anche metodo per tentativi, consiste nel cercare di indovinare una soluzione in base alla 
propria esperienza e di tentare di dimostrare tale soluzione per induzione.
\subsection{Metodo delle ricorrenze comuni}
Questo metodo di risoluzione delle equazioni ricorsive, detto anche metodo dell'esperto, mette a disposizione dei teoremi che permettono di risolvere 
facilmente ampie classi di equazioni di ricorrenza.
\subsubsection{Ricorrenze lineari con partizione bilanciata}
\paragraph{Teorema}
Siano $a$ e $b$ costanti intere tali che $a\ge 1$ e $b\ge 2$ e $c$ e $\beta$ costanti reali tali che $c>0$ e $\beta\ge 0$. Sia $T(n)$ l'equazione di 
ricorrenza nella forma:
\begin{equation*}
T(n)=
\begin{cases}
aT(\frac{n}{b})+cn^\beta\quad\quad & n>1\\
d & n\le 1
\end{cases}
\end{equation*}
Posto $\alpha=\frac{\log a}{\log b}=\log_b a$ allora:
\begin{equation*}
T(n)=
\begin{cases}
\Theta(n^\alpha) \quad\quad & \alpha>\beta\\ 
\Theta(n^\alpha\log n) & \alpha=\beta\\ 
\Theta(n^\beta) & \alpha<\beta
\end{cases}
\end{equation*}
\paragraph{Dimostrazione}\mbox{}
Si supponga dapprima per semplicit\`a che $n$ sia una potenza di $b$: $n = b^k$ per un qualche $k$ intero. Sostituendolo successivamente si ottiene: 
\begin{align*}
	T(n) &= aT(b^{k-1}) + cb^{k\beta} \\
	     &= a(aT(b^{k-2}) + cb^{(k-1)\beta}) + cb^{k\beta}\\
	     &= a(a(aT(b^{k-3}) + cb^{(k-2)\beta}) + cb^{(k-1)\beta}) + cb^{k\beta}\\
	     &\ \vdots \\
	     &= a(a(\cdots a(aT(1) + cb^{\beta}) + cb^{2\beta})+\cdots+cb^{(k-1)\beta})+cb^{k\beta}\\
	     &= a^kd + ca^{k-1}b^\beta + ca^{k-2}b^\beta + \cdots + cab^{(k-1)\beta} + cb^{k\beta}\\
	     &= a^kd + cb^{k\beta}\biggl[\biggl(\frac{a}{b^\beta}\biggr)^{k-1} + \biggl(\frac{a}{b^\beta}\biggr)^{k-2} + \cdots \biggl(\frac{a}{b^\beta}\biggr)^{2} + \frac{a}{b^\beta} + 1
	     	\biggr]
\end{align*}
Si noti che: $a^k = a^{\frac{\log n}{\log b}}= 2^{\frac{\log a \log n}{\log b}} = n^{\frac{\log a}{\log b}} = n^\alpha$. Inoltre $a = b^\alpha$ in quanto $\alpha = \frac{\log a}{\log b}$.
Ponendo $q = \frac{a}{b^\beta} = \frac{b^\alpha}{b^\beta} = b^{\alpha - \beta}$, la relazione pu\`o essere scritta come: 
$$ T(n) = n^\alpha d + cb^{k\beta}[q^{k-1} + q^{k-2} + \cdots + q^2 + q + 1]$$
Si individuano ora tre casi a seconda che $\alpha$ sia maggiore, uguale o minore di $\beta$ e che pertanto $q$ sia maggiore, uguale o minore di $1$. 
\subparagraph{$\mathbf{\alpha > \beta}$}
\begin{align*}
	T(n) &= n^\alpha d + cb^{k\beta}\biggl(\frac{q^k - 1}{q -1}\biggr)\\
	     &\le n^\alpha d + \frac{cb^{k\beta}q^k}{q-1}\\
	     &= n^\alpha d + \frac{ca^k}{q-1}\\
	     &=  n^\alpha\biggl(d + \frac{c}{q-1}\biggr)
\end{align*}
Pertanto $T(n) = O(n^\alpha)$.
\subparagraph{$\mathbf{\alpha = \beta}$}
\begin{align*}
	T(n) &= n^\alpha d + cb^{k\beta}(1+1+\cdots+1)\\
	     &= n^\alpha d +cb^\beta k\\
	     &= n^\alpha\biggl(d+c\frac{\log n}{\log b}\biggr)
\end{align*}
Pertanto $T(n) = O(n^\alpha\log n)$.
\subparagraph{$\mathbf{\alpha<\beta}$}
\begin{align*}
	T(n) &= n^\alpha d + cb^{k\beta}\biggl(\frac{1-q^k}{1-q}\biggr)\\
	     &= n^\alpha d + cb^{k\beta}\biggl(\frac{1}{1-q}\biggr)\\
	     &=n^\alpha d + \frac{cn^\beta}{1-q}
\end{align*}
Pertanto $T(n) = O(n^\beta)$.
\subparagraph{Conclusione:} Nel caso in cui $n$ non sia una potenza di $b$ si pu\`o considerare un problema di dimensione pi\`u grande con $n'$ tale che sia la potenza pi\`u piccola di 
$b$ maggiore di $n$. Essendo $n'\le bn$ e $b$ \`e costante, la dimensione aumenta di un valore costante e rimane valida. Analizzando il caso pessimo la soluzione vale anche nel caso
in cui valga il $\le$ anzich\`e il $=$. Rimane valida anche se $T(n)$ \`e uguale ad una costante per $n\le h$ con $h$ costante intera maggiore di $1$ o con ceil e floor di $\frac{n}{b}$.
\subsubsection{Estensione delle ricorrenze lineari con partizione bilanciata}
Sia $a\ge 1$, $b>1$ e $f(n)$ asintoticamente positiva e sia 
\begin{equation*}
T(n)=
\begin{cases}
aT(\frac{n}{b})+f(n)\quad\quad & n>1\\
d & n\le 1
\end{cases}
\end{equation*}
Sono dati tre casi.
\begin{itemize}
\item $\exists \varepsilon >0: f(n)=O(n^{\log_ba-\varepsilon})\Rightarrow T(n)=\Theta(n^{\log_ba})$.
\item $f(n)=\Theta(n^{\log_ba})\Rightarrow T(n)=\Theta(f(n)\log n)$.
\item $\exists\varepsilon>0:f(n)=\Omega(n^{\log_ba+\varepsilon})\land \exists c:0<c<1, \exists m>0:af(\frac{n}{b})\le cf(n),\forall n\ge m\Rightarrow T(n)=
\Theta(f(n))$.
\end{itemize}
\subsubsection{Ricorrenze lineari di ordine costante}
Siano $a_1,\dots,a_n$ costanti intere non negative, con $h$ costante positiva, $c>0$ e $\beta\ge 0$ costanti reali e $T(n)$ una relazione di ricorrenza 
nella forma:
\begin{equation*}
T(n)=
\begin{cases}
\sum\limits_{1\le i\le h}a_iT(n-i)+cn^\beta\quad\quad& n>m\\
\Theta(1) & n\le m\le h
\end{cases}
\end{equation*}
Posto $a=\sum\limits_{1\le i\le h}a_i$ allora:
\begin{itemize}
\item $T(n)=\Theta(n^{\beta+1})$ se $a=1$.
\item $T(n)=\Theta(a^nn^{\beta})$ se $a\ge 2$.
\end{itemize}
\paragraph{Dimostrazione}
Si supponga che ci sia un solo coefficiente non nullo, ovvero esista $j, 1\le j\le h$ tale che $a_j = a$, mentre $a_i = 0$ per $1\le i \le h \land i\neq j$. Si assuma per semplicit\`a
$n = m + kj$ per un $k$ intero. Sostituendo successivamente si ottiene: 
\begin{align*}
	T(n) &= aT(n-j) + cn^\beta \\
	     &= a(aT(n-2j)+c(n-j)^\beta)+cn^\beta\\
	     &\ \vdots\\
	     &= a(a\cdots a(a(T(m)+c(m+j)^\beta)+c(m+2j)^\beta)+\cdots+c(n-j)^\beta)+cn^\beta\\
	     &= a^kT(m) + \sum\limits_{0\le v\le k-1} a^vc(n-vj)^\beta\\
	     &= a^kT(m)+cn^\beta\sum\limits_{0\le v \le k-1} a^v
\end{align*}
\subparagraph{$\mathbf{a = 1}$}
\begin{align*}
	T(n) &\le T(m) + cn^\beta(1+1+\cdots+1)\\
	     &= T(m) + cn^\beta k\\
	     &= T(m) + cn^\beta\frac{n-m}{j}
\end{align*}
Pertanto $T(n)=O(n^{\beta+1})$.
\subparagraph{$\mathbf{a\ge 2}$}
\begin{align*}
	T(n) &= a^kT(m)+cn^\beta\biggl(\frac{a^k-1}{a-1}\biggr)\\
	     &\le a^k[T(m)+cn^\beta]\\
	     &= a^{\frac{n-m}{j}}[T(m)+cn^\beta]
\end{align*}
Pertanto $T(n) = O(a^nn^\beta)$.
\subparagraph{Conclusione:} Si consideri ora il caso pi\`u generale in cui $\prod\limits_{i =1}^h a_i \neq0$, ovvero tutti gli $h$ coefficienti sono non nulli con $h>1$. In tal caso
necessariamente si ha $\sum\limits_{i =1}^h a_i \ge 2$, pertanto:
\begin{align*}
	T(n) &= \sum\limits_{1\le i\le h} a_iT(n-i)+cn^\beta\\
	     &\le \sum\limits_{1\le i \le h} a_iT(n-1) + cn^\beta\\
	     &= aT(n-1) + cn^\beta
\end{align*}
In questo modo ci si riconduce al caso precedente in cui $j = h = 1$ e $a_j = a\ge 2$. La dimostrazione \`e analoga anche per $n\neq m+kj$.
\section{Relazione tra complessit\`a di un problema e di un algoritmo}
Un problema ha complessit\`a $O(f(n))$ se esiste un algoritmo che lo risolve con complessit\`a $O(f(n))$. Un problema ha complessit\`a $\Omega(f(n))$ se
tutti gli algoritmi che lo risolvono hanno complessit\`a $\Omega(f(n))$.
\subsection{Complessit\`a in tempo di un algoritmo}
La quantit\`a di tempo richiesta per input di dimensione $n$:
\begin{itemize}
\item $O(f(n))$: per tutti gli input l'algoritmo costa al pi\`u $f(n)$.
\item $\Omega(f(n))$: per tutti gli input l'algoritmo costa almeno $f(n)$.
\item $\Theta(f(n))$: per tutti gli input l'algoritmo richiede $f(n)$.
\end{itemize}
\subsection{Complessit\`a in tempo di un problema computazionale}
La quantit\`a di tempo richiesta per input di dimensione $n$:
\begin{itemize}
\item $O(f(n))$: complessit\`a del miglior algoritmo che risolve il problema.
\item $\Omega(f(n))$: dimostrare che nessun algoritmo pu\`o risolvere il problema in un tempo inferiore a $\Omega(f(n))$.
\item $\Theta(f(n))$: algoritmo ottimo.
\end{itemize}
\section{Valutare algoritmi in base alla tipologia di input}
In alcuni casi gli algoritmi si comportano in maniera diversa in base a caratteristiche dell'input. Conoscerle in anticipo permette di scegliere il miglior
algoritmo per la situazione. 
\subsection{Tipologie di analisi}
\begin{itemize}
\item Analisi del caso pessimo: il tempo di esecuzione \`e il limite superiore al tempo di esecuzione per un qualisasi input. 
\item Analisi del caso medio: molto difficile in quanto si deve trovare una distribuzione uniforme degli input.
\item Analisi del caso ottimo: ha senso se si conoscono caratteristiche dell'input.
\end{itemize}
\subsection{Algoritmi di ordinamento}
Il problema di ordinamento \`e rappresentato da una sequenza $A=a_1, \dots, a_n$ di valori in input e d\`a in output una sequenza $B=b_1, \dots, b_n$ 
permutazione di $A$ tale per cui $\forall 0<i<n-1, b_i\le b_{i+1}$.
\subsubsection{Selection sort}
Cerco il minimo e lo metto nella posizione corretta, riducendo il problema ai restanti $n-1$ valori.
\input{Pseudocodice/0_SelectionSort}

\input{Pseudocodice/1_Min}
\paragraph{Complessit\`a}
\begin{equation*}
\sum\limits_{i=1}^{n-1}(n-i)=\sum\limits_{i=1}^{n-1}i=\dfrac{n(n-1)}{2}=n^2-\dfrac{n}{2}=O(n^2)
\end{equation*}
\subsubsection{InsertionSort}
Algoritmo efficiente per ordinare piccoli insiemi. In cui si prende l'i-esimo elemento e lo si mette nella posizione corretta rispetto agli elementi
precedenti, proseguendo fino alla fine.
\input{Pseudocodice/2_InsertionSort}
\paragraph{Correttezza}\mbox{}\\
L'invariante di ciclo in un ciclo \emph{for} viene verificata dopo la modifica della variabile e prima della verifica. L'invariante da considerare \`e: all'inizio del ciclo $i$-esimo
il sottovettore $A[1\dots i -1]$ \`e ordinato.
\begin{itemize}
	\item \emph{Inizializzazione}: inizialmente $j=2$, il sottoarray dato da $A[1\dots 1]$ \`e ordinato.
	\item \emph{Conservazione}: sapendo che il subarray $A[1\dots j-1]$ \`e oridnato, il ciclo while sposta tutti gli elementi maggiori di $A[j]$ fino a una posizione $k$ inclusa
		di una posizione verso destra preservando l'ordine ma duplicando il valore in posizione $k$. Tutti i valori precedenti $k$ sono minori di $A[j]$ e ordinati, tutti i 
		valori dopo $k$ sono maggiori di $A[j]$ e ordinati, pertanto inserendo $A[j]$ in posizione $k$ si dimostra l'invariante. 
	\item \emph{Terminazione}: alla fine del ciclo, $j = n + 1$, l'invariante mostra come l'array intero $A[1\dots j - 1] = A[1\dots n]$ \`e ordinato.
\end{itemize}
\paragraph{Complessit\`a}
La complessit\`a \`e determinata dal numero di confronti effettuati per ogni elemento $i\in [2, n]$. Detto il numero di confronti $t_i$, il numero totale dipendente dalla posizione 
iniziale \`e $\sum\limits_{i=2}^n t_i$.
\subparagraph{Caso ottimo}
L'array \`e ordinato e pertanto $t_i = 1 \forall i$ in quanto $A[j-1] = A[i]\le A[j], \forall j$, pertanto il \emph{while} esce subito. La complessit\`a \`e $O(n)$. 
\subparagraph{Caso pessimo}
L'array \`e ordinato nell'ordine inverso, tutti gli elementi in $A[1\dots i - j]$ sono superiori ad $A[i]$, pertanto vengono spostati, il numero di confronti \`e pari a $i$: 
$\sum\limits_{i=2}^n(i-1) = (\sum\limits_{i = 2}^{n-1}i)-1 = \frac{n(n+1)}{2}$, pertanto il risultato finale sar\`a $T(n) = an^2+bn+c = O(n^2)$. 
\subparagraph{Caso medio}
In media met\`a degli elementi in $A[1\dots i - 1]$ sono maggiori di $A[i]$, e met\`a minori, pertanto per i singoli elementi si ottiene $\sum\limits_{i = 2}^n\frac{i}{2}=\frac{n(n+1)}{4}
-1$, il costo finale $T(n) = an^2+bn+c = O(n^2)$. 
\subsubsection{MergeSort}
Il mergeSort si basa sulla tecnica di divide-et-impera in quanto divide il vettore di $n$ elementi in due sottovettori di $\frac{n}{2}$ elementi, chiama il
mergeSort ricorsivamente su quei due elementi e unisce (merge) le due sequenze ordinate. Si sfrutta il fatto che i due sottovettori sono gi\`a ordinati per
ordinare pi\`u velocemente.
\paragraph{\emph{merge()}}
\input{Pseudocodice/3_Merge}
\subparagraph{Costo computazionale} $O(n)$ in quanto tutti gli elementi di entrambi gli array devono essere visitati almeno una volta. 
\myparagraph{\emph{mergeSort()}}
\input{Pseudocodice/4_MergeSort}
\subparagraph{Costo computazionale} Si assuma per semplificare che $n = 2^k$ in modo che $k=\log n$ e tutti i sottovettori hanno dimensioni di potenze 
esatte di due. Si ottiene cos\`i l'equazione di ricorrenza:
\begin{equation*}
T(n)=\begin{cases}
c\quad\quad & n=1\\
2T(\frac{n}{2})+dn & n>1\\
\end{cases}
\end{equation*}
Si noti come ad ogni chiamata ricorsiva si svolga un'operazione di merge di costo $O(n)$ e $k$ vari tra $0$ e $\log n$. Si ottiene pertanto 
$O(\sum\limits_{i=0}^ks^i\frac{n}{2^i})=O(\sum\limits_{i=0}^kn)=O(kn)\Leftrightarrow O(n\log n)$. 
\section{Analisi ammortizzata}
Si intende per analisi ammortizzata una tecnica di analisi di complessit\`a che valuta il tempo per eseguire nel caso pessimo una sequenza di operazioni 
su una struttura dati. Esistono operazioni pi\`u o meno costose e se le operazioni costose sono meno frequenti allora il loro costo pu\`o essere 
ammortizzato da quelle meno costose. A differenza dell'analisi del caso medio \`e deterministica su operazioni multiple e verifica il caso pessimo.
\subsection{Metodi per l'analisi ammortizzata}
\subsubsection{Metodo dell'aggregazione}
In questo metodo si calcola la complessit\`a $T(n)$ per eseguire $n$ operazioni in sequenza nel caso pessimo. Grazie alla sequenza si considera l'evoluzione
della struttura dati data una sequenza di operazioni, considerando la sequenza pessima e sommando insieme tutte le complessit\`a individuali. 
Successivamente questo $T(n)$ viene diviso per il numero di operazioni in modo da verificare la complessit\`a ammortizzata di un'operazione nella sequenza.
\subsubsection{Metodo degli accantonamenti}
Alle operazioni vengono assegnati costi ammortizzati che possono essere minori o maggiori del loro costo effettivo. Questa differenza \`e dovuta al fatto
che le operazioni meno costose vengono caricate di un costo aggiuntivo detto credito: $costo\ ammortizzato=costo\ effettivo+credito\ prodotto$. Questo 
credito accumulato viene speso dalle operazioni pi\`u costose: $costo\ ammortizzato=costo\ effettivo-credito\ consumato$. Si deve dimostrare che la somma
dei costi ammortizzato $a_i$ \`e un limite superiore alla somma dei costi effettivi: $\sum\limits_{i=1}^nc_i\le\sum\limits_{i=1}^na_i$ e che il valore 
cos\`i ottenuto \`e "poco costoso". Considerando il caso pessimo, la dimostrazione deve valere per tutte le sequenze e il credito dopo la t-esima operazione
deve essere sempre positivo: $\sum\limits_{i=1}^ta_i\le\sum\limits_{i=1}^tc_i\ge 0$.
\subsubsection{Metodo del potenziale}
Lo stato del sistema viene descritto attraverso una funzione di potenziale $\Phi(D)$. Le operazioni meno costose devono incrementare $\Phi(D)$ e quelle
pi\`u costose decrementarlo. Il costo ammortizzato \`e pari al costo effettivo sommato alla differenza di potenziale: $a_i=c_i+\Phi(D_i)-\Phi(D_{i-1})$.
Per una sequenza di operazioni di lunghezza $n$, se $\Phi(D_n)-\Phi(D_0)\ge 0$ il costo ammortizzato $A$ \`e un limite superiore al costo reale.
