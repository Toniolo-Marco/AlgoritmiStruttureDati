\chapter{Strutture dati speciali}
\section{Code con priorit\`a}
Una coda con priorit\`a \`e una struttura dati astratta simile ad una coda in cui ogni elemento inserito possiede una priorit\`a. Si dividono in min-priority 
queue in cui l'estrazione avviene per valore crescente di priorit\`a e max-priority queue, in cui l'estrazione avviene per valore decrescente di priorit\`a.
Le operazioni permesse sono di inserimento in coda, estrazione dell'elemento con priorit\`a di valore min/max e di modifica di priorit\`a di un elemento 
inserito.
\subsection{Specifica}
\input{Pseudocodice/66_}
\subsection{Implementazioni}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Metodo & \makecell{Lista o vettore \\non ordinato} & \makecell{Lista\\ordinata} & \makecell{Vettore\\ordinato} & \makecell{Albero\\RB}\\
\hline
\emph{min()} & $O(n)$ & $O(1)$ & $O(1)$ & $O(\log n)$\\
\hline
\emph{deleteMin()} & $O(n)$ & $O(1)$ & $O(n)$ & $O(\log n)$\\
\hline
\emph{insert()} & $O(n)$ & $O(n)$ & $O(n)$ & $O(\log n)$\\
\hline
\emph{decrease()} & $O(n)$ & $O(n)$ & $O(\log n)$ & $O(\log n)$\\
\hline
\end{tabular}
\end{center}
L'implementazione pi\`u comune \`e attraverso gli heap, strutture speciali che associano i vantaggi di un albero (esecuzioni in tempo $O(\log n)$) e 
vantaggi di un vettore (memorizzazione efficiente).
\subsection{Heap}
\subsubsection{Definizioni di alberi}
\paragraph{Albero binario perfetto}
In un albero binario perfetto tutte le foglie hanno la stessa altezza $h$, tutti i nodi interni hanno grado $2$ e dato $n$ il numero di nodi ha altezza
$h=\lfloor \log n\rfloor$. Data l'altezza $h$ ha numero di nodi $n=2^{h+1}-1$.
\paragraph{Albero binario completo}
In un albero binario completo tutte le foglie hanno altezza $h$ o $h-1$, tutti i nodi a livello $h$ sono accatastati a sinistra e tutti i nodi hanno grado 
$2$ tranne al pi\`u uno. Dato il numero di nodi $n$ ha altezza $h=\lfloor \log n\rfloor$.
\subsubsection{Alberi binari heap}
Un albero binario max-heap (min-heap) \`e un albero completo tale che il valore memorizzato nel nodo \`e maggiore (minore) dei valori memorizzati nei suoi
figli. Un albero heap non impone una relazione di ordinamento totale tra i figli di un nodo, pertanto \`e un ordinamento parziale. 
\subsubsection{Memorizzazione}
Gli alberi binari heap sono memorizzati tramite un vettore heap $A=[0, \dots, n-1]$ tale per cui: 
\begin{itemize}
\item La radice si trova all'indice $i=0$, ovvero \emph{root()=0}.
\item Il padre del nodo salvato all'indice $i$ si trova in \emph{p(i) = $\biggl\lfloor\dfrac{(i-1)}{2} \biggr\rfloor$}.
\item Il figlio sinistro del nodo salvato all'indice $i$ si trova in \emph{l(i) = $2i+1$}.
\item Il figlio destro del nodo salvato all'indice $i$ si trova in \emph{r(i) = $2i+2$}.
\end{itemize}
Propriet\`a max-heap sul vettore: 
$$A[i]\ge A[l(i)], A[i]\ge A[r(i)]$$ 
Propriet\`a min-heap sul vettore: 
$$A[i]\le A[l(i)], A[i]\le A[r(i)]$$
\subsubsection{\emph{heapsort()}}
L'\emph{heapsort()} ordina un max-heap "in-place" prima costruendo un max-heap nel vettore e poi spostando l'elemento max in ultima posizione, ripristinando
la propriet\`a max-heap. Utilizza pertanto le funzioni \emph{heapBuild()} che costruisce un max-heap a partire dal vettore non ordinato e \emph{ 
maxHeapRestore()} che ripristina le propriet\`a max-heap.
\paragraph{\emph{maxHeapRestore()}}
Riceve come input un vettore $A$ e un indice $i$, tale per cui gli alberi binari con radici $l(i)$ e $r(i)$ sono max-heap e il suo obiettivo \`e modificare
"in-place" il vettore $A$ in modo tale che l'albero binario con radice $i$ sia un max-heap. Essendo che ad ogni chiamata vengono eseguiti $O(1)$ confronti, 
e se il nodo $i$ non \`e massimo si chiama ricorsivamente sui figli e che l'esecuzione termina quando si raggiunge una foglia e l'altezza dell'albero \`e
pari a $\lfloor \log n\rfloor$ $T(n)=O(\log n)$.
\input{Pseudocodice/67_MaxHeapRestore}
\subparagraph{Dimostrazione della correttezza per induzione sull'altezza}
Al termine dell'esecuzione l'albero radicato in $A[i]$ rispetta la propriet\`a max-heap.
\begin{itemize}
\item Caso base $h=0$: se l'altezza \`e zero esiste un unico nodo che rispetta la propriet\`a.
\item Ipotesi induttiva: l'algoritmo funziona correttamente su tutti gli alberi di altezza inferiore ad $h$.
\item Passo induttivo:
\begin{itemize}
\item Caso 1: $A[i]\ge A[l(i)], A[i]\ge A[r(i)]$, allora l'albero radicato in $A[i]$ rispetta la propriet\`a di max-heap e l'esecuzione termina.
\item Caso 2: $A[l(i)] > A[i], A[r(i)]> A[i]$, viene fatto uno scambio $A[i]\leftrightarrow A[l(i)]$, dopo lo scambio $A[i]\ge A[l(i)], A[i]\ge A[r(i)]$, 
il sottoalbero radicato in $r(i)$ rimasto inalterato,  mentre il sottoalbero radicato in $l(i)$ pu\`o aver perso la propriet\`a, pertanto si svolge la 
chiamata ricorsiva su di esso che ha altezza minore di $h$, pertanto \`e corretto.
\item Caso 3: simmetrico rispetto al caso 2.
\end{itemize}
\end{itemize}
\paragraph{\emph{heapBuild()}}
Sia $A[1,\dots, n]$ un vettore da ordinare, tutti i nodi $A[\lfloor \frac{n}{2}\rfloor+1,\dots, n]$ sono foglie dell'albero e pertanto heap contenenti un 
elemento. La procedura \emph{heapBuild()} attraversa i restanti nodi dell'albero a partire da $\lfloor \frac{n}{2}\rfloor$ fino ad $1$ ed esegue 
\emph{maxHeapRestore()} su ognuno di essi. L'algoritmo ha complessit\`a $\Theta(\log n)$. 
\input{Pseudocodice/68_HeapBuild}
\subparagraph{Correttezza}
Si dimostra attraverso l'invariante di ciclo: all'inizio di ogni iterazione del ciclo \textbf{for} i nodi $[i+1,\dots, n]$ sono radice di uno heap.
\begin{itemize}
	\item Inizializzazione: all'inizio $i=\bigl\lfloor \frac{n}{2}\bigr\rfloor$. Si dimostri che tutti i nodi da $\bigl\lfloor\frac{n}{2}\bigr\rfloor + 1$ sono foglie: si 
		supponga per assurdo che $\bigl\lfloor \frac{n}{2}\bigr\rfloor+1$ non sia una foglia, pertanto esiste almeno il figlio sinistro 
		$2\bigl\lfloor \frac{n}{2}\bigr\rfloor+2$ che fa parte dello heap, con indice superiore ad $n$, pertanto assurdo. La dimostrazione vale per tutti gli indici successivi 
		e tutti i nodi $i+1\dots n$ sono heap banali.
	\item Conservazione: per ipotesi induttiva si suppone che al passo $i$ tutti i nodi $i+1, i+2, \dots, n$ siano radici di heap. \`E quindi possibile applicare 
		\emph{maxHeapRestore()} al nodo $i$ in quanto $2i< 2i+1< n$ sono entrambi radici di heap. Al termine dell'iterazione tutti i nodi $[1,\dots, n]$ sono radici di heap.
	\item Conclusione: al termine $i=0$, pertanto il nodo $1$ \`e radice di uno heap. 
\end{itemize}
\paragraph{Implementazione \emph{heapsort}}
Il primo elemento contiene il massimo, viene collocato in fondo, l'elemento in fondo viene collocato in testa, si chiama \emph{maxHeapRestore()} per 
ripristinare la situazione e la dimensione dello heap viene progressivamente ridotta. L'algoritmo ha complessit\`a $\Theta(n\log n)$ in quanto 
\emph{heapBuild()} ha costo $\Theta(n)$ e la chiamata di \emph{maxHeapRestore()} costa $\Theta(\log i)$ in un heap con $i$ elementi. Pertanto 
$T(n)=\sum\limits_{i=2}^n\log i+\Theta(n)=\Theta(n\log n)$.
\input{Pseudocodice/69_Heapsort}
\subparagraph{Correttezza}
Si dimostra la correttezza per invariante di ciclo. Al passo $i$ il sottovettore $A[i+1,\dots, n]$ \`e ordinato, $A[1,\dots, i]\le A[i+1, \dots, n]$ e 
$A[1]$ \`e la radice di un vettore heap di dimensione $i$. Si dimostri per induzione su $i$: 
\begin{itemize}
	\item \emph{Inizializzazione}: al passo $i=n$ il sottovettore $A[n+1\dots n]$ \`e vuoto. Dopo aver eseguito \emph{heapbuild()}, $A[1]$ \`e la radice di un vettore heap di 
		dimensione $n$.
	\item \emph{Conservazione}: ad ogni passo $i$, il nodo in posizione $A[1]$ \`e il massimo fra i nodi rimanenti $A[1\dots i]$ ed \`e minore di tutti i nodi in $A[i+1\dots n]$. 
	\item \emph{Conclusione}: l'algoritmo termina quando $i = 1$ che vuol dire che il vettore dato dai nodi $A[2\dots n]$ \`e ordinato e $A[1]\le A[2\dots n]$, pertanto il vettore
		\`e ordinato. 
\end{itemize}
\subparagraph{Complessit\`a}
Vengono compiuti $n$ passi ognuno dei quali richiede $O(\log n)$ passi, pertanto il costo totale \`e $O(n\log n)$. Si noti la differenza con l'analisi di complessit\`a di 
\emph{heapBuild()} in quanto questo funziona dal basso, viene costruito solo una volta uno heap di altezza $\log n$, in questo caso \emph{maxHeapRestore()} viene eseguito $\frac{n}{2}$
volte con alberi di altezza $h = O(\log n)$, pertanto il costo totale \`e $O(n\log n)$. 
\subsection{Implementazione di code con priorit\`a}
Si vedr\`a l'implementazione di una min-priority queue, visto che la max-priority \`e speculare. 
\subsubsection{Memorizzazione}
\input{Pseudocodice/70_}

\input{Pseudocodice/71_Swap}
\subsubsection{Inizializzazione}
\input{Pseudocodice/72_}
\subsubsection{Inserimento}
\input{Pseudocodice/73_Insert}
\subsubsection{\emph{minHeapRestore()}}
\input{Pseudocodice/74_MinHeapRestore}
\subsubsection{Cancellazione e lettura minimo}
\input{Pseudocodice/75_}

\input{Pseudocodice/76_}
\subsubsection{Decremento priorit\`a}
\input{Pseudocodice/77_Decrease}
\subsubsection{Complessit\`a}
Tutte le operazioni che modificano gli heap devono sistemare la propriet\`a heap lungo un cammino radice-foglia (\emph{deleteMin()}) oppure lungo un cammino
nodo-radice (\emph{insert()}, \emph{decrease()}). Essendo l'altezza $\lfloor\log n\rfloor$ il costo di tali operazioni \`e $O(\log n)$.
\begin{center}
\begin{tabular}{|c|c|}
\hline
Operazione & Costo\\
\hline
\emph{insert()}& $O(\log n)$\\
\hline
\emph{deleteMin()} & $O(\log n)$\\
\hline
\emph{min()} & $\Theta(1)$\\
\hline
\emph{decrase()} & $O(\log n)$\\
\hline
\end{tabular}
\end{center}
\section{Insiemi disgiunti - Merge-find set}
In alcune applicazioni si \`e interessati a gestire una collezione $S=\{S_1, \dots, S_k\}$ di insiemi dinamici disgiunti tali che:
\begin{itemize}
\item $\forall i, j: i\neq j\Rightarrow S_i\cap S_j = \emptyset$.
\item $\bigcup\limits_{i =1}^k S_i = S$.
\end{itemize}
Un esempio di questi insiemi sono le componenti di un grafo. Le operazioni fondamentali sono: 
\begin{itemize}
\item Creare $n$ insiemi disgiunti, ognuno composto da un unico elemento.
\item \emph{merge()}: unire pi\`u insiemi.
\item \emph{find()}: identificare l'insieme a cui appartiene un elemento.
\end{itemize}
\subsubsection{Rappresentante}
Ogni insieme \`e identificato da un rappresentante univoco, un qualunque membro dell'insieme. Le operazioni di ricerca del rappresentante su uno stesso insieme devono restituire sempre lo stesso oggetto. Il
rappresentante pu\`o cambiare solo in caso di unione.
\subsection{Specifica}
\input{Pseudocodice/78_}
\subsection{Componenti connesse dinamiche}
\begin{multicols}{2}
Per trovare le componenti connesse di un grafo non orientato dinamico si inizia con componenti connesse costituite da un unico vertice, per ogni $(u, v)\in E$ si esegue \emph{merge(u, v)} e alla fine ogni insieme
disgiunto rappresenta una componente connessa.
\input{Pseudocodice/79_Cc}
\end{multicols}
Questo algoritmo ha complessit\`a $O(n)+m$ operazioni \emph{merge()}, interessante per la capacit\`a di gestire grafi dinamici.
\subsection{Implementazione con insieme di liste}
Ogni insieme viene rappresentato da una lista concatenata: il primo oggetto di una lista \`e il rappresentante dell'insieme e ogni elemento della lista contiene un oggetto, un puntatore all'elemento successivo e
un puntatore al rappresentante. 
\subsubsection{Operazione \emph{find(x)}}
Si restituisce il rappresentante di $x$, l'operazione \emph{find(x)} richiede tempo $O(1)$.
\subsubsection{Operazione \emph{merge(x, y)}}
Si appende la lista che contiene $y$ alla lista che contiene $x$ modificando i puntatori ai rappresentanti nella lista appesa. Nel caso pessimo per $n$ operazioni il costo \`e $O(n^2)$, il costo ammortizzato \`e
$O(n)$.
\subsection{Implementazione con insieme di alberi}
Ogni insieme viene rappresentato da un albero, ogni nodo dell'albero contiene un oggetto e un puntatore al padre, la radice \`e il rappresentante dell'insieme e ha un puntatore a s\`e stessa.
\subsubsection{Operazione \emph{find(x)}}
Risale la lista dei padri di $x$ fino a trovare la radice e restituisce la radice come rappresentante. Ha costo $O(n)$ nel caso pessimo
\subsubsection{Operazione \emph{merge(x, y)}}
Si aggancia l'albero radicato in $y$ a $x$ modificando il puntatore al padre di $y$, con costo $O(1)$.
\subsection{Tecniche euristiche}
Si dice algoritmo euristico un particolare tipo di algoritmo progettato per risolvere un problema pi\`u velocemente se i metodi classici siano troppo lenti, trovare una soluzione approssimata qualora i metodi 
classici falliscano nel trovare una soluzione esatta. 
\subsubsection{Liste: euristica sul peso}
Si utilizza per diminuire il costo dell'operazione \emph{merge()} si memorizza nella lista l'informazione sulla loro lunghezza e si aggancia la lista pi\`u corta a quella pi\`u lunga. La lunghezza della lista pu\`o 
essere mantenuta in tempo $O(1)$. Tramite analisi ammortizzata si nota come il costo di $n-1$ operazioni merge \`e $O(n\log n)$, pertanto il costo ammortizzato delle singole operazioni \`e $O(\log n)$.
\subsubsection{Alberi: euristica sul rango}
Si utilizza per diminuire il costo dell'operazione \emph{find(x)}: ogni nodo mantiene informazioni sul proprio rango, \emph{rank[x]} di un nodo $x$ \`e il numero di archi del cammino pi\`u lungo fra $x$ e una 
foglia sua discendente, ovvero l'altezza del sottoalbero associato al nodo. L'obiettivo \`e mantenere bassa l'altezza degli alberi. Un albero \emph{Mfset} con radice $r$ ottenuto tramite euristica sul
rango ha almeno $2^{rank[r]}$ nodi. Inoltre lo stesso albero con radice $r$ e $n$ nodi ha altezza inferiore a $\log n$. 
\begin{multicols}{2}
\input{Pseudocodice/80_}
\columnbreak
\input{Pseudocodice/81_Merge}
\end{multicols}
\subsubsection{Alberi: euristica di compressione dei cammini}
\begin{multicols}{2}
Nell'operazione \emph{find(x)} l'albero viene appiattito in modo che ricerche successive di $x$ siano svolte in $O(1)$.
\input{Pseudocodice/82_Find}
\end{multicols}
\subsubsection{Alberi: euristica sul rango e compressione dei cammini}
Applicando entrambe le euristiche il rango non \`e pi\`u l'altezza del nodo ma il limite superiore alla sua altezza. Non viene calcolato il rango corretto. Il costo ammortizzato di $m$ operazioni merge-find in un 
insieme di $n$ elementi \`e $O(m\alpha(n))$, dove $\alpha(n)$ o funzione inversa di Ackermann cresce lentamente. 
