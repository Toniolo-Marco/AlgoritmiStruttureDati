\chapter{Tecniche risolutive per problemi intrattabili}
Per la risoluzione di alcuni problemi si rende necessario rinunciare a delle caratteristiche:
\begin{itemize}
	\item Generalit\`a: algoritmi pseudo-polinomiali che funzionano solo per alcuni casi particolari dell'input.
	\item Ottimalit\`a: algoritmi di approssimazione che garantiscono di ottenere soluzioni vicine alla soluzione ottimale.
	\item Efficienza: algoritmi esponenziali branch-\&-bound che limitano lo spazio di ricerca con un'accurata potatura.
	\item Formalit\`a: algoritmi euristici di solito basati su tecniche greedy o di ricerca locale che forniscano sperimentalmente risultati buoni.
\end{itemize}
\section{Subset-Sum}
\paragraph{Problema}
Dati un vettore $A$ contenente $n$ interi positivi e un intero positivo $k$, verificare se esiste un sottoinsieme $S\subseteq\{1, \dots, n\}$ tale che $\sum\limits_{i\in S}a[i] = k$.
\paragraph{Soluzioni}
Utilizzando backtracking si \`e risolta la versione di ricerca di questo problema mentre questa \`e la versione decisionale, su cui ci si concentrer\`a. 
\subsection{Programmazione dinamica}
Si definisce una tabella booleana $DP[0\dots n][0\dots k]$ tale per cui $DP[i][r]$ \`e uguale a \emph{\textbf{true}} se esiste un sottoinsieme dei primi $i$ valori memorizzati in $A$
la cui somma \`e pari a $r$, \emph{\textbf{false}} altrimenti.
$$DP[i][r] = 
\begin{cases}
	true \quad\quad\quad & r = 0\\
	false & r > 0 \land i = 0\\
	DP[i - 1][r] & r > 0 \land i > 0 \land A[i] > r\\
	DP[i - 1][r] \lor DP[i - 1][r - A[i]] & r > 0 \land i > 0 \land A[i]\le r\\
\end{cases}$$
\input{Pseudocodice/143_subSumDim}
\subsection{Backtracking}
\input{Pseudocodice/144_subSumBack}
\subsection{Memoization}
\input{Pseudocodice/145_subSumMem}
\subsection{Complessit\`a}
\begin{itemize}
	\item Programmazione dinamica: $\Theta(nk)$.
	\item Backtracking: $O(2^n)$.
	\item Memoization con dizionario $O(nk), O(2^n)$.
\end{itemize}
$O(nk)$ non \`e una complessit\`a polinomiale in quanto $k$ \`e parte dell'input, non una sua dimensione. Questo viene rappresentato con $t = \lceil\log k\rceil$ cifre binarie, pertanto
$O(nk) = O(n\cdot 2^t)$, che \`e esponenziale.
\section{Problemi fortemente, debolmente $\mathbf{\mathbb{NP}}$-completi}
\subsubsection{Dimensioni del problema}
Dato un problema decisionale $R$ e una sua istanza $I$, si definisce la dimensione $d$ di $I$ come la lunghezza della stringa che codifica $I$, il valore $\#$ \`e il pi\`u grande numero 
intero che appare in $I$.
\subsection{Problemi}
\subsubsection{Cricca (CLIQUE)}
Dati un grafo non orientato e un intero $k$ si verifichi se esiste un sottoinsieme di almeno $k$ nodi tutti mutualmente adiacenti.
\subsubsection{Commesso viaggiatore (TSP)}
Date $n$ citt\`a e una matrice simmetrica $d$ di distanze positive, dove $d[i][j]$ \`e la distanza tra $i$ e $j$, trovare un percorso che, partendo da una qualsiasi citt\`a
attraversi ogni citt\`a esattamente una volta e ritorni alla citt\`a di partenza in modo che la distanza totale percorsa sia minima.
\subsubsection{Esempi}
\begin{tabular}{|c|c|c|c|}
	\hline
	\textbf{Nome} & $\mathbf{I}$ & $\mathbf{\#}$ & $\mathbf{d}$ \\
	\hline
	SUBSET-SUM & $\{n, k, A\}$ & $\max\{n, k, \max(A)\}$ & $O(n\log\#)$\\
	\hline
	CLIQUE & $\{n, m, k, G\}$ & $\max\{n, m, k\}$ & $O(n + m + \log\#)$\\
	\hline
	TSP & $\{n, k, d\}$ & $\max\{n, k, \max(d)\}$ & $O(n^2\log\#)$ \\
	\hline
\end{tabular}
\subsection{Problema fortemente $\mathbf{\mathbb{NP}}$-completo}
Sia $R_{pol}$ il problema $R$ ristretto a quei dati d'ingresso per i quali $\#$ \`e limitato superiormente da $p(d)$ con $p$ una funzione polinomiale in $d$. $R$ \`e fortemente 
$\mathbb{NP}$-completo se $R_{pol}$ \`e $\mathbb{NP}$-completo.
\subsubsection{Cricca (CLIQUE)}
Si mostri come questo problema \`e fortemente $\mathbb{NP}$-completo: si prenda $k\le n$ (altrimenti la risposta \`e \textbf{false}), $\# = \max\{n, m, k\} = \max\{n, m\}$, 
$d = O(n+m+\log\#)=O(n+m)$, pertanto $\# = \max\{n, m\}$ \`e limitato superiormente da $O(n+m)$ quindi il problema ristretto \`e identico a CLIQUE che \`e $\mathbb{NP}$-completo.
\subsubsection{Commesso viaggiatore (TSP)}
Si dimostri che TSP \`e fortemente $\mathbb{NP}$-completo. Per assurdo si supponga che TSP sia debolmente $\mathbb{NP}$-completo, allora esiste una soluzione pseudo-polinomiale  e la 
si usa per risolvere un problema $\mathbb{NP}$-completo in tempo polinomiale, assurdo a meno che $\mathbb{P}=\mathbb{NP}$. 
\subsubsection{Circuito hamiltoniano (HAMILTONIAN-CIRCUIT)}
Si deve verificare se dato un grafo non orientato $G$ esista un circuito che attraversi ogni nodo una e una sola volta. 
\paragraph{Complessit\`a}
HAMILTONIAN-CIRCUIT \`e $\mathbb{NP}$-completo e uno dei $21$ problemi elencati nell'articolo di Karp.
\subsubsection{Dimostrazione che TSP \`e fortemente $\mathbf{\mathbb{NP}}$-completo}
Sia $G=(V, E)$ un grafo non orientato e si definisca una matrice di distanze a partire da $G$:
$$d[i][j] = 
\begin{cases}
	1 & (i, j)\in E\\
	2 & (i, j)\not\in E\\
\end{cases}$$
Il grafo $G$ ha un circuito hamiltoniano se e solo se \`e possibile trovare un percorso da commesso viaggiatore di costo $n$, se esistesse un algoritmo pseudopolinomiale $A$ per TSP, 
HAMILTONIAN-CIRCUIT potrebbe essere risolto da $A$ in tempo polinomiale, che \`e un assurdo.
\subsection{Problema debolmente $\mathbf{\mathbb{NP}}$-completo}
Se un problema $\mathbb{NP}$-completo non \`e fortemente $\mathbb{NP}$-completo allora \`e debolmente $\mathbb{NP}$-completo.
\subsubsection{Somma di sottoinsieme (SUBSET-SUM)}
Dati un vettore $A$ contenente $n$ interi positivi e un intero positivo $k$, verificare se esiste un sottoinsieme $S\subseteq\{1\dots n\}$ tale che $\sum\limits_{i\in S}a[i] = k$.
\paragraph{Si verifichi che SUBSET-SUM \`e debolmente $\mathbf{\mathbb{NP}}$-completo}
Si consideri $\forall A[i]\le k$ con i valori maggiori di $k$ esclusi, se $k=O(n^c)$, allora $\# = \max\{n, k, a_1,\dots, a_n\}=O(n^c)$. La soluzione basata su programmazione dinamica
ha complessit\`a $O(nk)=O(n^{c+1})$, quindi in $\mathbb{P}$, pertanto SUBSET-SUM non \`e fortemente $\mathbb{NP}$-completo. 
\subsection{Algoritmi pseudo-polinomiali}
Un algoritmo che risolve un problema $R$ per qualsiasi dato $I$ d'ingresso in tempo $P(\#, d)$, con $p$ funzione polinomiale in $\#$ e $d$ ha complessit\`a pseudo-polinomiale. Si noti
come gli algoritmi per SUBSET-SUM basati su programmazione dinamica e memoization sono pseudo-polinomiali. 
\subsubsection{Teorema}
Nessun problema fortemente $\mathbb{NP}$-completo pu\`o essere risolto da un algoritmo pseudo-polinomiale a meno che non sia $\mathbb{P}=\mathbb{NP}$.
\subsection{ESEMPI}
\subsubsection{Partizione (PARTITION)}
Dato un vettore $A$ contenente $n$ interi positivi verificare se esiste un sottoinsieme $S\subseteq\{1\dots n\}$ tale che $\sum\limits_{i\in S}A[i] = \sum\limits_{i\not\in S}A[i]$. Si
noti come questo problema \`e debolmente $\mathbb{NP}$-completo in quanto \`e possibile ridurlo a SUBSET-SUM scegliendo come valore $k$ la met\`a di tutti i valori presenti.
\subsubsection{3-Partizione (3-PARTITION)}
Dati $3n$ interi $\{a_1, \dots, a_{3n}\}$ verificare se esiste una partizione in $n$ triple $T_1, \dots, T_n$ tale che la somma dei tre elementi di ogni $T_j$ \`e la stessa per $1\le j
\le n$. Si noti come questo problema non \`e debolmente $\mathbb{NP}$-completo in quanto non esiste un algoritmo pseudo-polinomiale per risolverlo.
\section{Algoritmi di approssimazione}
Si noti come i problemi pi\`u interessanti sono in forma di ottimizzazione e se il problema di decisione \`e $\mathbb{NP}$-completo non sono noti algoritmi polinomiali che trovano
soluzioni ammissibili pi\`u o meno vicine a quella ottima. Se \`e possibile dimostrare un limite superiore o inferiore al rapporto fra la soluzione trovata e la soluzione ottima allora
tali algoritmi vengono detti algoritmi di approssimazione. 
\paragraph{Definizione}
Dato un problema di ottimizzazione con funzione di costo non negativa $c$, un algoritmo si dice di $\alpha(n)$-approssimazione se fornisce una soluzione ammissibile $x$ il cui costo
$c(x)$ non si discosta dal costo $c(x^*)$ della soluzione ottima $x^*$ per pi\`u di un fattore $\alpha(n)$ per un qualunque input di dimensione $n$: 
\begin{align*}
	c(x*)\le c(x)\le\alpha(n)c(x^*) & \alpha(n)>1 & \text{Minimizzazione}\\
	\alpha(n)c(x^*)\le c(x)\le c(x^*) & \alpha(n)<1 & \text{Massimizzazione}
\end{align*}
$\alpha(n)$ pu\`o essere una costante valida per tutti gli $n$. Identificare un valore $\alpha(n)$ e dimostrare che l'algoritmo lo rispetta \`e ci\`o che rende un buon algoritmo un
algoritmo di approssimazione.
\subsection{Bin packing}
Sia un vettore $A$ contenente $n$ interi positivi (volumi degli oggetti), un intero positivo $k$ (la capacit\`a di una scatola tale che $\forall i:A[i]\le k$), si vuole trovare una
partizione di $\{1, \dots, n\}$ nel minimo numero di sottoinsiemi disgiunti tali che $\sum\limits_{i\in S}A[i]\le k$ per ogni insieme $S$ della partizione. 	
\subsubsection{Algoritmo FIRST-FIT}
Gli oggetti sono considerati in un ordine qualsiasi e ciascun oggetto \`e assegnato alla prima scatola che lo pu\`o contenere tenuto conto di quanto spazio \`e stato occupato della 
stessa. Si noti come \`e un algoritmo greedy. 
\paragraph{$\mathbf{\alpha}(n)$}
Sia $N>1$ il numero di scatole usate da FIRST-FIT in quanto se $N=1$ FIRST-FIT \`e ottimale. Il numero minimo di scatole $N^*$ \`e limitato da:
$$N^*\le\dfrac{\sum\limits_{i = 1}^n A[i]}{k}$$
Non possono esserci due scatole riempite meno della met\`a:
$$N<\dfrac{\sum\limits_{i = 1}^n A[i]}{\frac{k}{2}}$$
Si ha quindi::
$$N<\dfrac{\sum\limits_{i = 1}^n A[i]}{\frac{k}{2}}=2\dfrac{\sum\limits_{i = 1}^n A[i]}{k}\le 2N^*=\alpha(n)N^*$$
\paragraph{Variante First-fit decreasing}
Gli oggetti sono considerati in ordine non decrescente. Si noti come queste sono dimostrazioni di limiti superiori per il fattore $\alpha(n)$ e per casi particolari l'approssimazione
pu\`o essere migliore. 
\subsection{Commesso viaggiatore con disuguaglianze particolari (triangolari o $\mathbf{\Delta}$-TSP)}
Siano date $n$ citt\`a e le distanze positive $d[i][j]$ tra esse tali per cui vale la regola delle disuguaglianze triangolari:
$$d[i][j]\le d[i][k]+d[k][j] \forall i, j, k: 1\le i, j, k\le n$$
Si deve trovare un percorso che partendo da una qualsiasi citt\`a attraversi ogni citt\`a esattamente una volta e ritorni alla citt\`a di partenza in modo che la distanza complessiva 
sia minima. 
\subsubsection{$\mathbf{\Delta}$-TSP \`e $\mathbf{\mathbb{NP}}$-completo}
Si dimostri che HAMILTONIAN-CIRCUIT $\le_p\ \Delta$-TSP. Sia $G=(V, E)$ un grafo non orientato e si definisca la matrice di distanze a partire da $G$:
$$d[i][j] = 
\begin{cases}
	1 & (i, j)\in E\\
	2 & (i, j)\not\in E\\
\end{cases}$$
Il grafo $G$ ha un circuito hamiltoniano se e solo se \`e possibile trovare un percorso da commesso viaggiatore lungo $n$. Valgono le disuguaglianze triangolari:
$$d[i][j] \le 2\le d[i][k] + d[k][j]$$
\subsubsection{Algoritmo di approssimazione}
Si interpreti $\Delta$-TSP come il problema di trovare un circuito hamiltoniano di peso minimo su un grafo completo. Considerando un circuito hamiltoniano e cancellando un
suo arco si ottiene un albero di copertura. 
\paragraph{Teorema}
\subparagraph{Enunciato}
Qualunque circuito hamiltoniano $\pi$ ha costo $c(\pi)$ superiore al costo $mst$ di un albero di copertura di peso minimo, ovvero $mst < c(\pi)$.
\subparagraph{Dimostrazione}
Si supponga per assurdo che esista un circuito hamiltoniano $\pi$ di costo $c(\pi)\le mst$, togliendo un arco si ottiene un albero di copertura con peso inferiore $mst' < c(\pi)\le mst$,
che \`e una contraddizione in quanto $mst$ \`e il costo minimo fra tutti gli alberi di copertura. 
\paragraph{Algoritmo}
Si individua un minimo albero di copertura di peso $mst$ e se ne percorrono gli archi due volte, prima in un senso e poi nell'altro in modo da visitare ogni citt\`a almeno una volta, 
la distanza complessiva \`e $2\cdot mst$ ma non \`e un circuito hamiltoniano. Si evita di passare per le citt\`a gi\`a visitate saltandole e per la disuguaglianza triangolare il
costo $c(\pi)$ \`e inferiore o uguale a $2\cdot mst$, pertanto $c(\pi)\le 2\cdot mst< 2\cdot c(\pi^*)\Rightarrow \alpha(n) = 2$, dove $c(\pi^*)$ \`e il costo del circuito hamiltoniano
ottimo. 
\paragraph{Note}
La complessit\`a dell'algoritmo \`e $O(n^2\log n)$ con $O(n^2\log n)$ per l'algoritmo di Kruskal e $O(n)$ per la visita in profondit\`a del MST raddoppiato con $2n$ archi. Esistono
grafi perversi per cui il fattore di approssimazione tende al valore $2$. L'algoritmo di Christofides ha un fattore di approssimazione di $\frac{3}{2}$. 
\subsubsection{Non approssimabilit\`a di TSP}
Non esiste alcun algoritmo di $\alpha(n)$-approssimazione per TSP tale che $c(x')\le sc(x^*)$ con $s\ge 2$ intero positivo a meno che non sia $\mathbb{P} = \mathbb{NP}$.
\section{Algoritmi euristici}
Gli algoritmi euristici forniscono una soluzione ammissibile non necessariamente ottima e non necessariamente approssimata, utilizzando tecniche greedy o di ricerca locale.
\subsection{Approccio greedy}
\subsubsection{TSP - shortest edges first}
In questo algoritmo si ordinano gli archi per pesi non decrescenti, si aggiungono archi alle soluzioni seguendo questo ordine fino a che non sono stati aggiunti $n-1$ archi, dove $n$
\`e il numero di nodi. Per aggiungere un arco occorre verificare che per ciascuno dei suoi nodi non sian gi\`a stati scelti due archi e che non si formino circuiti MFSET. A questo punto
si \`e trovata una catena hamiltoniana e si chiude il circuito aggiungendo l'arco tra i due nodi estremi della catena. Ha un costo computazionale di  $O(n^2\log n)$ per l'ordinamento 
degli archi.\\
\input{Pseudocodice/146_greedyTsp}
\subsubsection{TSP - Nearest neighbor}
Si parte da una citt\`a, si seleziona come prossima citt\`a quella pi\`u vicina e si va avanti cos\`i evitando citt\`a gi\`a visitate e quando si sono visitate tutte le citt\`a si 
torna nella citt\`a di partenza. Si pu\`o lavorare direttamente sulla matrice. Ha costo computazionale $O(n^2)$.
\subsubsection{Conclusioni}
Si noti come la soluzione cos\`i ottenuta si pu\`o utilizzare come base di partenza per un algoritmo branch-\&-bound e pu\`o essere migliorata ancora tramite ricerca locale. 
\subsection{Approccio ricerca locale}
\subsubsection{TSP}
Sia $\pi$ un circuito hamiltoniano del grafo completo derivante dal problema TSP e si consideri il seguente intorno:
\begin{align*}
	I_2(\pi) = &\{\pi': \pi' \text{ottenuto da } \pi\text{ cancellando due archi non consecutivi del} \\
		   &\text{circuito e sostituendoli con due archi esterni al circuito}\}
\end{align*}
Si noti come $|I_2(\pi)| = \frac{n(n-1)}{2-n}$ in quanto ci sono $\frac{n(n-1)}{2}$ coppie di archi nel circuito e $n$ di esse sono consecutive e una volta spezzato un circuito esiste
un solo modo per riconnetterlo. Il costo per esaminare $I_2(\pi)$ \`e $O(n^2)$. 
\section{Branch-\&-bound}
Per risolvere un problema di ottimizzazione $\mathbb{NP}$-arduo si pu\`o modificare la procedura \emph{enumeration()} vista nella sezione su backtrack in modo da potare certe sequenze
di scelte incapaci di generare la soluzione ottima. Si pu\`o assumere senza perdere troppa generalit\`a che il problema sia di minimizzazione, ogni sequenza di scelte abbia un costo non
negativo e ogni scelta aggiunta alle scelte gi\`a effettuate non faccia diminuire il costo della soluzione parziale cos\`i costruita. 
\subsection{Upper bound}
Durante l'enumerazione si mantengono informazioni sulla miglior soluzione ammissibile $minSol$ e il suo costo $minCost$, che costituisce un limite superiore per il costo della soluzione
minima. 
\subsection{Lower bound}
Si supponga di avere a disposizione una opportuna funzione lower bound \emph{lb($<$dati problema$>$, S, i, $<$dati parziali$>$)} che dipenda dalla sequenza di scelte fatte $S[1\dots i]$
e garantisca che tutte le soluzioni ammissibili generabili facendo nuove scelte abbiano costo $\ge lb()$.
\subsubsection{Potatura}
Se \emph{lb()} \`e maggiore o uguale a $minCost$ allora si pu\`o evitare di generare ed esplorare il sottoalbero delle scelte radicato in tal nodo.
\subsection{Note}
Si noti come questo metodo non migliori la complessit\`a superpolinomiale della procedura \emph{enumeration()} ma ne abbassa drasticamente il tempo di esecuzione in pratica 
dipendentemente dalla funzione \emph{lb()} che deve approssimare il pi\`u possibile il costo della soluzione ottima. 
\subsection{Schema generale}
\input{Pseudocodice/147_branchBoundGenerale}
\subsection{Commesso viaggiatore}
Sia $n$ il numero di citt\`a e $d[h][k]$ la distanza intera positiva fra le citt\`a $h$ e $k$. Al passo $i$-esimo sono state fatte le scelte $S[1\dots i]$ prese dall'insieme 
$\{1,\dots n\}$, un percorso ammissibile che espande $S[1\dots i]$ deve attraversare le citt\`a $S[1\dots i]$, passare da $S[i]$ ad una qualsiasi delle rimanenti $n-i$ citt\`a, 
attraversare queste ultime citt\`a in un ordine qualsiasi e da una di queste ritornare a $S[1]$. 
\subsubsection{Lower bound}
Si calcola la distanza percorsa fino ad ora:
$$cost[i] = 
\begin{cases}
	0 & i = 1\\
	cost[i - 1] + d[S[i - 1][S[i]] & i > 1\\
\end{cases}$$
Il lower bound della distanza per uscire da $S[i]$ ($O(n)$):
$$out = \min\limits_{h\not\in S}\{d[S[i][h]\}$$
Il lower bound della distanza per tornare a $S[1]$ ($O(n)$):
$$back = \min\limits_{h\not\in S}\{d[h][S[1]]\}$$
Il lower bound della distanza percorsa per attraversare una qualsiasi citt\`a $h$ delle $n-i$ citt\`a ancora da attraversare provenendo da e dirigendosi verso un'altra di queste $n-i$
citt\`a ($O(n^3)$):
$$transfer[h] = \min\limits_{p, q\not\in S}\{d[p][h] + d[h][q] : h\neq p\neq q\}, \forall h\not\in S$$
Se $i<n$ un possibile lower bound \emph{d, S, i} calcola la somma del costo per arrivare al nodo $S[i]$ gi\`a speso, del lower bound \emph{out} del costo per andare dal nodo $S[i]$ ad un
qualunque altro nodo, del lower bound per attraversare i nodi non contenuti in $S$, del lower bound \emph{back} del costo per tornare dal nodo $S[1]$ da un qualunque altro nodo:
$$lb(d, S, i) = cost[i] + out +\biggl\lceil\dfrac{\sum\limits_{h\not\in S}transfer[h]}{2}\biggr\rceil + back$$
\subsubsection{Implementazione}
\input{Pseudocodice/148_branchBoundTSP}
Si noti come \emph{minCost} \`e una variabile globale e invece di inizializzarla a $+\infty$ si pu\`o scegliere una permutazione a caso. Per evitare che lo stesso circuito sia generato 
pi\`u volte si parte da un nodo fissato. 
\paragraph{Possibili miglioramenti}
\`E possibile variare l'ordine di visita dell'albero delle scelte (DFS contro Best-first), \`e possibile variare il meccanismo di branching sui nodi, archi, eccetera ,\`e possibile 
cercare dei lower bound pi\`u stretti. 
